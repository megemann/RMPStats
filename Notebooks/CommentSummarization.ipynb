{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\austi\\machinelearning\\RateMyProfessorStats\\Notebooks\n",
      "[{'key': '203815-100', 'name': 'PSYCH100'}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "# Now load the data\n",
    "professors_df = pd.read_csv('../UMassReviews/professors_df_clean.csv')\n",
    "reviews_df = pd.read_csv('../UMassReviews/reviews_df_clean_sentiment.csv')\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def clean_and_parse_json(x):\n",
    "    if not isinstance(x, str):\n",
    "        return x\n",
    "    try:\n",
    "        # First attempt: direct JSON parse\n",
    "        return json.loads(x)\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            # Second attempt: Replace single quotes with double quotes\n",
    "            x = x.replace(\"'\", '\"')\n",
    "            return json.loads(x)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                # Third attempt: Use ast.literal_eval (safer than eval)\n",
    "                import ast\n",
    "                return ast.literal_eval(x)\n",
    "            except:\n",
    "                print(f\"Failed to parse: {x[:100]}...\")  # Print first 100 chars\n",
    "                return {}  # Return empty dict if all parsing attempts fail\n",
    "\n",
    "\n",
    "reviews_df['class_identifiers'] = reviews_df['class_identifiers'].apply(clean_and_parse_json)\n",
    "print(reviews_df['class_identifiers'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\austi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\austi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "His lectures are basically him saying \"This should be obvious\" while teaching students concepts instead of carefully taking time to offer thorough explanations. Wilson was clearly lacking in social skills, and he held grudges against students who were genuinely trying to learn something new. Discussions and office hours were also poorly run, but he refused to take the class's weaknesses as a reflection on his pedagogy. Gotcha questions on exams, apathetic,disinterested, there aren't enough adjectives to describe the way this man treats people and runs his classroom.\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def summarize_text(text, num_sentences=4):\n",
    "    stemmer = PorterStemmer()  # Initialize the NLTK stemmer\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = LsaSummarizer()\n",
    "    summarizer.stemmer = stemmer  # Set the NLTK stemmer for the summarizer\n",
    "    summary = summarizer(parser.document, num_sentences)\n",
    "    return \" \".join([str(sentence) for sentence in summary])\n",
    "\n",
    "def get_all_reviews_for_professor(professor_id):\n",
    "    review_list = reviews_df[reviews_df['tid'] == professor_id]['comment'].tolist()\n",
    "    return \" \".join(review_list)\n",
    "\n",
    "def get_raw_reviews_for_professor(professor_id):\n",
    "    return reviews_df[reviews_df['tid'] == professor_id]\n",
    "\n",
    "print(summarize_text(get_all_reviews_for_professor(2936075)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However the class is a lot of work you MUST attend all lectures and keep up with readings to do well. You'll enjoy going to lecture, even though it' an 8AM (don't  skip, because a large chunk of test material centers around case studies discussed, videos shown, and diagnostic criteria presented in class. He makes the class interesting by sharing stories of his actual cases and provides three review sessions a week if you need help or extra credit. Didn't live up to my expectations after what I read on here...Nice guy, bit of an ego, but knows his stuff.\n"
     ]
    }
   ],
   "source": [
    "# Summarize by class\n",
    "def get_reviews_for_class(class_id):\n",
    "    review_list = []\n",
    "    for index, review in reviews_df.iterrows():  # Use iterrows to iterate over DataFrame rows\n",
    "        class_identifiers = review['class_identifiers']\n",
    "        if isinstance(class_identifiers, list):  # Ensure class_identifiers is a list\n",
    "            for item in class_identifiers:\n",
    "                if item.get('key') == class_id:\n",
    "                    review_list.append(review['comment'])\n",
    "    return \" \".join(review_list)\n",
    "\n",
    "def get_raw_reviews_for_class(class_id):\n",
    "    filtered_reviews = reviews_df[reviews_df['class_identifiers'].apply(lambda x: isinstance(x, list) and any(item.get('key') == class_id for item in x))]\n",
    "    return filtered_reviews\n",
    "\n",
    "print(summarize_text(get_reviews_for_class('83082-380')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However the class is a lot of work you MUST attend all lectures and keep up with readings to do well. You'll enjoy going to lecture, even though it' an 8AM (don't  skip, because a large chunk of test material centers around case studies discussed, videos shown, and diagnostic criteria presented in class. He makes the class interesting by sharing stories of his actual cases and provides three review sessions a week if you need help or extra credit. Didn't live up to my expectations after what I read on here...Nice guy, bit of an ego, but knows his stuff.\n",
      "His lectures are very engaging and makes it easy to get up early to go to class. His lectures are so interesting. Halgin's class is the best! Very funny as well for abnormal psych his class was very interesting and was one of my favorite classes that i have taken.\n",
      "I wish he was a professor for more psych classes because the amount I learned reflects how well he taught the material in a fun and interesting way, and made me want to go to class on a regular basis. We got there early just to get a seat (the auditorium was ALWAYS packed) Never missed a class, and the grades are earned, not too bad if you put in some effort, which is easy becase the material is so engaging. No Comments he's very experienced and knows a lot about his field which makes his class interesting No Comments No Comments No Comments only class i never missed one of He talks about himself a lot, but he's a great teacher. No Comments I NEVER missed this class it was so awsome Awesome professor, and gives the best and most interesting lectures No Comments No Comments Really good prof.  His lectures are interesting and guest lectures are great!\n",
      "I wish he was a professor for more psych classes because the amount I learned reflects how well he taught the material in a fun and interesting way, and made me want to go to class on a regular basis. Best class I have ever taken at Umass When I went to my adviser saying that I wanted to take abnormal with a different professor last spring, she suggested I wait and take it in the fall with Professor Halgin. We got there early just to get a seat (the auditorium was ALWAYS packed) Never missed a class, and the grades are earned, not too bad if you put in some effort, which is easy becase the material is so engaging. No Comments I NEVER missed this class it was so awsome Awesome professor, and gives the best and most interesting lectures No Comments No Comments Really good prof.  His lectures are interesting and guest lectures are great!\n"
     ]
    }
   ],
   "source": [
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "def summarize_text_compare(text, summarizer_type='lsa', sentences_count=4, language=\"english\"):\n",
    "    stemmer = PorterStemmer()  # Initialize the NLTK stemmer\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(language))\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - text: string, the text to summarize\n",
    "    - summarizer_type: string, type of summarizer ('lsa', 'lexrank', 'luhn', or 'textrank')\n",
    "    - sentences_count: int, number of sentences in summary\n",
    "    - language: string, language of text\n",
    "    \n",
    "    Returns:\n",
    "    - string, the summarized text\n",
    "    \"\"\"\n",
    "    \n",
    "    # Choose summarizer\n",
    "    if summarizer_type.lower() == 'lsa':\n",
    "        summarizer = LsaSummarizer()\n",
    "    elif summarizer_type.lower() == 'lexrank':\n",
    "        summarizer = LexRankSummarizer()\n",
    "    elif summarizer_type.lower() == 'luhn':\n",
    "        summarizer = LuhnSummarizer()\n",
    "    elif summarizer_type.lower() == 'textrank':\n",
    "        summarizer = TextRankSummarizer()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid summarizer type\")\n",
    "    \n",
    "    # Add stop words\n",
    "    summarizer.stemmer = stemmer\n",
    "    \n",
    "    # Create summary\n",
    "    summary = summarizer(parser.document, sentences_count)\n",
    "    \n",
    "    # Join sentences and return\n",
    "    return \" \".join([str(sentence) for sentence in summary])\n",
    "\n",
    "for item in ['lsa', 'lexrank', 'luhn', 'textrank']:\n",
    "    print(summarize_text_compare(get_reviews_for_class('83082-380'), item))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average_sentiment': np.float64(0.36357141514265673), 'summary_sentiment': 0.20844155844155843}\n",
      "{'average_sentiment': np.float64(-0.0009831377042025218), 'summary_sentiment': 0.02121212121212121}\n"
     ]
    }
   ],
   "source": [
    "def sumy_sentiment_by_class(class_id):\n",
    "    # Get the reviews for the specified class\n",
    "    reviews = get_raw_reviews_for_class(class_id)\n",
    "    # Calculate the average sentiment of the reviews\n",
    "    import numpy as np\n",
    "    average_sentiment = np.mean(reviews['sentiment_polarity'].tolist())\n",
    "\n",
    "    from textblob import TextBlob\n",
    "    summary_sentiment = TextBlob(summarize_text_compare(get_reviews_for_class(class_id), 'lsa')).sentiment.polarity\n",
    "\n",
    "    return {'average_sentiment': average_sentiment, 'summary_sentiment': summary_sentiment}\n",
    "\n",
    "print(sumy_sentiment_by_class('83082-380'))\n",
    "\n",
    "def sumy_sentiment_by_professor(professor_id):\n",
    "    # Get the reviews for the specified professor\n",
    "    reviews = get_raw_reviews_for_professor(professor_id)\n",
    "\n",
    "    # Calculate the average sentiment of the reviews\n",
    "    import numpy as np\n",
    "    average_sentiment = np.mean(reviews['sentiment_polarity'].tolist())\n",
    "\n",
    "    from textblob import TextBlob\n",
    "    summary_sentiment = TextBlob(summarize_text_compare(get_all_reviews_for_professor(professor_id), 'lsa')).sentiment.polarity\n",
    "\n",
    "    return {'average_sentiment': average_sentiment, 'summary_sentiment': summary_sentiment}\n",
    "\n",
    "print(sumy_sentiment_by_professor(2936075))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sumy_sentiment_by_professor:   0%|          | 0/1000 [00:00<?]c:\\Users\\austi\\machinelearning\\RateMyProfessorStats\\.venv\\Lib\\site-packages\\sumy\\summarizers\\lsa.py:76: UserWarning: Number of words (1680) is lower than number of sentences (1692). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "Processing sumy_sentiment_by_professor: 100%|██████████| 1000/1000 [02:21<00:00]\n",
      "Processing sumy_sentiment_by_class: 100%|██████████| 5486/5486 [1:55:41<00:00]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Professor Summary Evaluation\n",
      "Mean Absolute Error: 0.12208193490171151\n",
      "Median Absolute Error: 0.0986419331367494\n",
      "Standard Deviation of Absolute Error: 0.09374860007190475\n",
      "Class Summary Evaluation\n",
      "Mean Absolute Error: 0.08643856197440637\n",
      "Median Absolute Error: 0.0522152627465128\n",
      "Standard Deviation of Absolute Error: 0.1067269912822949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if os.path.exists('../UmassReviews/professor_errors.pkl') and os.path.exists('../UmassReviews/class_errors.pkl'):\n",
    "    # Load error data from CSV files\n",
    "    professor_errors = pd.read_pickle('../UmassReviews/professor_errors.pkl')\n",
    "    class_errors = pd.read_pickle('../UmassReviews/class_errors.pkl')\n",
    "else:\n",
    "    def summary_evaluation(summary_functions):\n",
    "\n",
    "        def run_summary_function(summary_function, data_list):\n",
    "            evaluation_list = []\n",
    "            from tqdm import tqdm\n",
    "            progress_bar = tqdm(total=len(data_list), desc=f\"Processing {summary_function.__name__}\", bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')\n",
    "            for item in data_list:\n",
    "                evaluation_list.append(summary_function(item))\n",
    "                progress_bar.update(1)\n",
    "            progress_bar.close()  # Close the progress bar after completion\n",
    "            \n",
    "            error_list = []\n",
    "            absolute_error_list = []\n",
    "            for item in evaluation_list:\n",
    "                error_list.append(item['average_sentiment'] - item['summary_sentiment'])\n",
    "                absolute_error_list.append(abs(item['average_sentiment'] - item['summary_sentiment']))\n",
    "\n",
    "            return {'error_list': error_list, 'absolute_error_list': absolute_error_list}\n",
    "\n",
    "        classes = []\n",
    "\n",
    "        for review in reviews_df['class_identifiers']:\n",
    "            for item in review:\n",
    "                classes.append(item['key'])\n",
    "\n",
    "        unique_classes = list(set(classes))\n",
    "        return [run_summary_function(summary_functions[0], professors_df['id']), run_summary_function(summary_functions[1], unique_classes)]\n",
    "\n",
    "\n",
    "    errors = summary_evaluation([sumy_sentiment_by_professor, sumy_sentiment_by_class])\n",
    "    professor_errors = errors[0]\n",
    "    class_errors = errors[1]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "professor_absolute_error_list = professor_errors['absolute_error_list']\n",
    "mean_absolute_error = np.mean(professor_absolute_error_list)\n",
    "median_absolute_error = np.median(professor_absolute_error_list)\n",
    "std_dev_absolute_error = np.std(professor_absolute_error_list)\n",
    "print(\"Professor Summary Evaluation\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error}\")\n",
    "print(f\"Median Absolute Error: {median_absolute_error}\")\n",
    "print(f\"Standard Deviation of Absolute Error: {std_dev_absolute_error}\")\n",
    "\n",
    "class_absolute_error_list = class_errors['absolute_error_list']\n",
    "mean_absolute_error = np.mean(class_absolute_error_list)\n",
    "median_absolute_error = np.median(class_absolute_error_list)\n",
    "std_dev_absolute_error = np.std(class_absolute_error_list)\n",
    "\n",
    "print(\"Class Summary Evaluation\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error}\")\n",
    "print(f\"Median Absolute Error: {median_absolute_error}\")\n",
    "print(f\"Standard Deviation of Absolute Error: {std_dev_absolute_error}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error data saved to professor_errors.pkl and class_errors.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the error data to pickle files\n",
    "with open('professor_errors.pkl', 'wb') as f:\n",
    "    pickle.dump(professor_errors, f)\n",
    "    \n",
    "with open('class_errors.pkl', 'wb') as f:\n",
    "    pickle.dump(class_errors, f)\n",
    "\n",
    "print(\"Error data saved to professor_errors.pkl and class_errors.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Semantic Understanding (sentence-transformers):\n",
    "\n",
    "```\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "```\n",
    "\n",
    "Purpose: Converts text into meaningful numerical vectors that capture semantic meaning\n",
    "Use case: Helps group similar reviews together, even if they use different words\n",
    "Example: \"Great teacher\" and \"Excellent instructor\" would be recognized as similar\n",
    "B. Topic Modeling (BERTopic):\n",
    "```\n",
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic()\n",
    "```\n",
    "- Purpose: Automatically discovers themes/topics in reviews\n",
    "How it works: Groups similar discussions together and identifies representative words\n",
    "Example: Might find clusters like \"Teaching Style\", \"Grading Fairness\", \"Workload\"\n",
    "C. Sentiment Analysis (TextBlob):\n",
    "- Purpose: Determines the emotional tone of reviews\n",
    "Use: Helps separate positive and negative aspects of each theme\n",
    "Example: \"Tough but fair\" vs \"Unnecessarily difficult\"\n",
    "D. Summarization (Transformers):\n",
    "```\n",
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "```\n",
    "- Purpose: Creates concise summaries of grouped reviews\n",
    "Use: After finding themes, summarizes the main points\n",
    "Example: Converting 20 similar reviews about \"tough grading\" into one representative summary\n",
    "\n",
    "Process Flow:\n",
    "```\n",
    "class ReviewAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Initialize models\n",
    "        self.sentence_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        self.topic_model = BERTopic()\n",
    "        self.summarizer = pipeline(\"summarization\")\n",
    "\n",
    "    def process_reviews(self, reviews):\n",
    "        # 1. Initial Processing\n",
    "        # Clean and validate reviews\n",
    "        valid_reviews = [r for r in reviews if isinstance(r, str) and len(r.strip()) > 10]\n",
    "\n",
    "        # 2. Theme Discovery\n",
    "        # Find main topics using BERTopic\n",
    "        topics, probs = self.topic_model.fit_transform(valid_reviews)\n",
    "        \n",
    "        # 3. Sentiment Analysis\n",
    "        # Analyze sentiment for each theme\n",
    "        sentiments = {\n",
    "            topic: [\n",
    "                TextBlob(review).sentiment.polarity \n",
    "                for review, t in zip(valid_reviews, topics) \n",
    "                if t == topic\n",
    "            ]\n",
    "            for topic in set(topics)\n",
    "        }\n",
    "\n",
    "        # 4. Theme Summarization\n",
    "        # Group reviews by theme and summarize\n",
    "        theme_summaries = {}\n",
    "        for topic in set(topics):\n",
    "            theme_reviews = [r for r, t in zip(valid_reviews, topics) if t == topic]\n",
    "            if theme_reviews:\n",
    "                summary = self.summarizer(\" \".join(theme_reviews[:3]))\n",
    "                theme_summaries[topic] = summary\n",
    "\n",
    "```\n",
    "\n",
    "Key Features to Consider:\n",
    "A. Aspect-Based Analysis:\n",
    "```\n",
    "aspects = {\n",
    "    'teaching_style': ['lectures', 'explains', 'teaching'],\n",
    "    'workload': ['homework', 'assignments', 'projects'],\n",
    "    'grading': ['grades', 'tests', 'exams'],\n",
    "    'personality': ['helpful', 'understanding', 'approachable']\n",
    "}\n",
    "```\n",
    "- Purpose: Pre-defined categories to organize feedback\n",
    "Use: Helps ensure important aspects aren't missed\n",
    "B. Frequency Analysis:\n",
    "'''\n",
    "from collections import Counter\n",
    "def get_key_phrases(reviews, min_count=2):\n",
    "    phrases = []\n",
    "    for review in reviews:\n",
    "        blob = TextBlob(review)\n",
    "        phrases.extend(blob.noun_phrases)\n",
    "    return Counter(phrases)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtextblob\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\austi\\machinelearning\\RateMyProfessorStats\\.venv\\Lib\\site-packages\\bertopic\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimportlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bertopic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[0;32m      5\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbertopic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\austi\\machinelearning\\RateMyProfessorStats\\.venv\\Lib\\site-packages\\bertopic\\_bertopic.py:41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Models\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhdbscan\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UMAP\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m sklearn_version\n",
      "File \u001b[1;32mc:\\Users\\austi\\machinelearning\\RateMyProfessorStats\\.venv\\Lib\\site-packages\\umap\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m warn, catch_warnings, simplefilter\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mumap_\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UMAP\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n",
      "File \u001b[1;32mc:\\Users\\austi\\machinelearning\\RateMyProfessorStats\\.venv\\Lib\\site-packages\\umap\\umap_.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tril \u001b[38;5;28;01mas\u001b[39;00m sparse_tril, triu \u001b[38;5;28;01mas\u001b[39;00m sparse_triu\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsgraph\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumba\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistances\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msparse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\austi\\machinelearning\\RateMyProfessorStats\\.venv\\Lib\\site-packages\\numba\\__init__.py:105\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_support\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m carray, farray, from_dtype\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Re-export experimental\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Initialize withcontexts\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwithcontexts\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1091\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1190\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from bertopic import BERTopic\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings\n",
    "\n",
    "class ComprehensiveReviewAnalyzer:\n",
    "    def __init__(self, device=None):\n",
    "        \"\"\"Initialize all necessary models and configurations\"\"\"\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize models\n",
    "        print(\"Loading models...\")\n",
    "        self.sentence_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        self.summarizer = pipeline(\"summarization\", \n",
    "                                 model=\"facebook/bart-large-cnn\",\n",
    "                                 device=0 if self.device == 'cuda' else -1)\n",
    "        self.topic_model = BERTopic(language=\"english\", \n",
    "                                  calculate_probabilities=True,\n",
    "                                  verbose=True)\n",
    "        \n",
    "        # Define aspect categories\n",
    "        self.aspects = {\n",
    "            'teaching_style': ['lectures', 'explains', 'teaching', 'presentation', 'clarity'],\n",
    "            'difficulty': ['hard', 'easy', 'difficult', 'straightforward', 'challenging'],\n",
    "            'workload': ['homework', 'assignments', 'workload', 'projects', 'work'],\n",
    "            'grading': ['grades', 'tests', 'exams', 'quizzes', 'fair', 'marking'],\n",
    "            'personality': ['helpful', 'understanding', 'approachable', 'office hours', 'kind'],\n",
    "            'engagement': ['interesting', 'boring', 'engaging', 'interactive', 'enthusiasm']\n",
    "        }\n",
    "\n",
    "    def preprocess_reviews(self, reviews):\n",
    "        \"\"\"Clean and validate reviews\"\"\"\n",
    "        # Remove empty or invalid reviews\n",
    "        valid_reviews = [\n",
    "            str(r).strip() \n",
    "            for r in reviews \n",
    "            if isinstance(r, (str, float, int)) and str(r).strip()\n",
    "        ]\n",
    "        \n",
    "        # Remove very short reviews\n",
    "        valid_reviews = [r for r in valid_reviews if len(r.split()) >= 5]\n",
    "        \n",
    "        # Basic cleaning\n",
    "        cleaned_reviews = [\n",
    "            r.replace('\\n', ' ').replace('\\r', ' ').strip()\n",
    "            for r in valid_reviews\n",
    "        ]\n",
    "        \n",
    "        return cleaned_reviews\n",
    "\n",
    "    def extract_themes(self, reviews):\n",
    "        \"\"\"Extract main themes using BERTopic\"\"\"\n",
    "        try:\n",
    "            topics, probs = self.topic_model.fit_transform(reviews)\n",
    "            topic_info = self.topic_model.get_topic_info()\n",
    "            \n",
    "            # Get representative docs for each topic\n",
    "            topic_docs = {}\n",
    "            for topic in set(topics):\n",
    "                if topic != -1:  # Skip outlier topic\n",
    "                    topic_docs[topic] = [\n",
    "                        reviews[i] for i, t in enumerate(topics) if t == topic\n",
    "                    ]\n",
    "            \n",
    "            return {\n",
    "                'topic_info': topic_info,\n",
    "                'review_topics': topics,\n",
    "                'probabilities': probs,\n",
    "                'topic_docs': topic_docs\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error in theme extraction: {e}\")\n",
    "            return None\n",
    "\n",
    "    def analyze_sentiment_patterns(self, reviews):\n",
    "        \"\"\"Analyze sentiment patterns in reviews\"\"\"\n",
    "        sentiments = []\n",
    "        for review in reviews:\n",
    "            blob = TextBlob(review)\n",
    "            sentiments.append({\n",
    "                'polarity': blob.sentiment.polarity,\n",
    "                'subjectivity': blob.sentiment.subjectivity,\n",
    "                'text': review\n",
    "            })\n",
    "        \n",
    "        # Group by sentiment\n",
    "        sentiment_groups = {\n",
    "            'positive': [s for s in sentiments if s['polarity'] > 0.2],\n",
    "            'negative': [s for s in sentiments if s['polarity'] < -0.2],\n",
    "            'neutral': [s for s in sentiments if -0.2 <= s['polarity'] <= 0.2]\n",
    "        }\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats = {\n",
    "            'average_polarity': np.mean([s['polarity'] for s in sentiments]),\n",
    "            'sentiment_distribution': {\n",
    "                k: len(v) for k, v in sentiment_groups.items()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return {'groups': sentiment_groups, 'stats': stats}\n",
    "\n",
    "    def extract_aspect_sentiments(self, reviews):\n",
    "        \"\"\"Analyze sentiments for specific aspects\"\"\"\n",
    "        aspect_sentiments = {aspect: [] for aspect in self.aspects}\n",
    "        \n",
    "        for review in reviews:\n",
    "            review_lower = review.lower()\n",
    "            \n",
    "            # Check each aspect\n",
    "            for aspect, keywords in self.aspects.items():\n",
    "                # If any keyword is found, analyze sentiment for this aspect\n",
    "                if any(keyword in review_lower for keyword in keywords):\n",
    "                    sentiment = TextBlob(review).sentiment.polarity\n",
    "                    aspect_sentiments[aspect].append({\n",
    "                        'sentiment': sentiment,\n",
    "                        'text': review\n",
    "                    })\n",
    "        \n",
    "        # Calculate statistics for each aspect\n",
    "        aspect_stats = {}\n",
    "        for aspect, sentiments in aspect_sentiments.items():\n",
    "            if sentiments:\n",
    "                aspect_stats[aspect] = {\n",
    "                    'average_sentiment': np.mean([s['sentiment'] for s in sentiments]),\n",
    "                    'count': len(sentiments),\n",
    "                    'example_positive': next((s['text'] for s in sentiments \n",
    "                                           if s['sentiment'] > 0.2), None),\n",
    "                    'example_negative': next((s['text'] for s in sentiments \n",
    "                                           if s['sentiment'] < -0.2), None)\n",
    "                }\n",
    "        \n",
    "        return aspect_stats\n",
    "\n",
    "    def summarize_theme(self, reviews, max_length=130):\n",
    "        \"\"\"Summarize a group of related reviews\"\"\"\n",
    "        try:\n",
    "            # Combine reviews but limit total length\n",
    "            combined = \" \".join(reviews[:5])  # Take first 5 reviews as sample\n",
    "            summary = self.summarizer(combined, \n",
    "                                    max_length=max_length, \n",
    "                                    min_length=30, \n",
    "                                    do_sample=False)[0]['summary_text']\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            print(f\"Error in summarization: {e}\")\n",
    "            return None\n",
    "\n",
    "    def generate_comprehensive_analysis(self, reviews):\n",
    "        \"\"\"Generate a comprehensive analysis of all reviews\"\"\"\n",
    "        print(\"Starting comprehensive review analysis...\")\n",
    "        \n",
    "        # Preprocess reviews\n",
    "        cleaned_reviews = self.preprocess_reviews(reviews)\n",
    "        if not cleaned_reviews:\n",
    "            return {\"error\": \"No valid reviews found\"}\n",
    "        \n",
    "        print(f\"Analyzing {len(cleaned_reviews)} valid reviews...\")\n",
    "        \n",
    "        # Extract themes\n",
    "        themes = self.extract_themes(cleaned_reviews)\n",
    "        \n",
    "        # Analyze sentiments\n",
    "        sentiment_analysis = self.analyze_sentiment_patterns(cleaned_reviews)\n",
    "        \n",
    "        # Analyze aspects\n",
    "        aspect_analysis = self.extract_aspect_sentiments(cleaned_reviews)\n",
    "        \n",
    "        # Generate theme summaries\n",
    "        theme_summaries = {}\n",
    "        if themes and 'topic_docs' in themes:\n",
    "            for topic, docs in themes['topic_docs'].items():\n",
    "                theme_summaries[topic] = self.summarize_theme(docs)\n",
    "        \n",
    "        return {\n",
    "            'review_count': len(cleaned_reviews),\n",
    "            'themes': themes,\n",
    "            'sentiment_analysis': sentiment_analysis,\n",
    "            'aspect_analysis': aspect_analysis,\n",
    "            'theme_summaries': theme_summaries\n",
    "        }\n",
    "\n",
    "def format_analysis_results(analysis):\n",
    "    \"\"\"Format analysis results for readable output\"\"\"\n",
    "    if 'error' in analysis:\n",
    "        return analysis['error']\n",
    "    \n",
    "    output = []\n",
    "    output.append(f\"Analysis of {analysis['review_count']} Reviews\\n\")\n",
    "    \n",
    "    # Overall sentiment\n",
    "    sentiment_stats = analysis['sentiment_analysis']['stats']\n",
    "    output.append(\"Overall Sentiment:\")\n",
    "    output.append(f\"Average rating: {sentiment_stats['average_polarity']:.2f}\")\n",
    "    output.append(\"Distribution:\")\n",
    "    for sentiment, count in sentiment_stats['sentiment_distribution'].items():\n",
    "        output.append(f\"- {sentiment}: {count}\")\n",
    "    output.append(\"\")\n",
    "    \n",
    "    # Aspect analysis\n",
    "    output.append(\"Aspect Analysis:\")\n",
    "    for aspect, stats in analysis['aspect_analysis'].items():\n",
    "        output.append(f\"\\n{aspect.replace('_', ' ').title()}:\")\n",
    "        output.append(f\"- Mentioned in {stats['count']} reviews\")\n",
    "        output.append(f\"- Average sentiment: {stats['average_sentiment']:.2f}\")\n",
    "        if stats['example_positive']:\n",
    "            output.append(f\"- Positive example: {stats['example_positive'][:100]}...\")\n",
    "        if stats['example_negative']:\n",
    "            output.append(f\"- Negative example: {stats['example_negative'][:100]}...\")\n",
    "    \n",
    "    # Theme summaries\n",
    "    if analysis['theme_summaries']:\n",
    "        output.append(\"\\nMain Themes:\")\n",
    "        for topic, summary in analysis['theme_summaries'].items():\n",
    "            if summary:\n",
    "                output.append(f\"\\nTheme {topic}:\")\n",
    "                output.append(f\"- {summary}\")\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your reviews\n",
    "    reviews_df = pd.read_csv('path_to_your_reviews.csv')\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = ComprehensiveReviewAnalyzer()\n",
    "    \n",
    "    # Analyze reviews for a specific professor\n",
    "    professor_reviews = reviews_df[reviews_df['professor_id'] == 'PROF_ID']['comments'].tolist()\n",
    "    \n",
    "    # Generate and format analysis\n",
    "    analysis = analyzer.generate_comprehensive_analysis(professor_reviews)\n",
    "    formatted_output = format_analysis_results(analysis)\n",
    "    \n",
    "    print(formatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
